{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# GAME OF THRONES: A GRAPH EXPERIMENT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the extracted characters informations from pickles\n",
    "\n",
    "The dictionaries are loaded directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('characters.pickle','rb') as fp:\n",
    "    characters = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aliases': None, 'awoif_in_degree': 0, 'awoif_infobox_length': 0, 'awoif_links': ['Sandor_Clegane', 'Beric_Dondarrion', 'Anguy', 'Kyle', 'Thoros_of_Myr'], 'awoif_out_degree': 5, 'awoif_page_size': 22295, 'books': [3], 'category': 'appears', 'common_name': 'Dennet', 'fullname': None, 'short_name': 'Dennett', 'titles': None, 'url': 'Dennet', 'score': 0.028193470151365908}\n"
     ]
    }
   ],
   "source": [
    "print(characters[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the chapters and the book they belong to\n",
    "The books have been cut into chapters themselves into a book directory named GOT{x} where x is the book in order of publication.\n",
    "We read the whole directory to retain only text files. we create a chapter list containing the text of the chapter and the book index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 345 chapters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "\n",
    "chapters = []\n",
    "\n",
    "for root, dirs, files in os.walk('data'):\n",
    "    for name in files:\n",
    "        if name.endswith('txt'):\n",
    "            book = int(root.split('/')[-1][-1:])\n",
    "            with open(join(root,name),'r') as fp:\n",
    "                chapters.append((book,fp.read()))\n",
    "                \n",
    "print('found',len(chapters),'chapters')\n",
    "#print(chapters[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize a person entity : Get Most Probable Character for an incomplete NLP person entity\n",
    "Check partial entity against character properties set in the following order\n",
    "1. Check for __identity__\n",
    "    1. check _without_ the title\n",
    "        1. common name\n",
    "        2. short name\n",
    "        3. full name\n",
    "        4. aliases\n",
    "    2. check _with_ the title + name\n",
    "        1. common name\n",
    "        2. short name\n",
    "        3. full name\n",
    "        4. aliases\n",
    "2. Check for __partial__ inclusion\n",
    "    1. check _without_ the title\n",
    "        1. common name\n",
    "        2. short name\n",
    "        3. full name\n",
    "        4. aliases\n",
    "    2. check _with_ the title + name\n",
    "        1. common name\n",
    "        2. short name\n",
    "        3. full name\n",
    "        4. aliases\n",
    "\n",
    "If two candidate appear at the same level, we use the maximal distance scoring to determine the most valuable character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMVP(name, shortened, key, equal=True, title=False):\n",
    "    candidates = []\n",
    "        \n",
    "    for s in shortened:\n",
    "        if key != 'aliases':\n",
    "            if equal and s[key] is not None:\n",
    "                if not title:\n",
    "                    if name == s[key]:\n",
    "                        candidates.append(s)\n",
    "                elif s['titles'] is not None:\n",
    "                    for t in s['titles']:\n",
    "                        if name == t + ' ' + s[key]:\n",
    "                            candidates.append(s)\n",
    "                        \n",
    "            elif not equal and s[key] is not None:\n",
    "                if not title:\n",
    "                    if name in s[key]:\n",
    "                        candidates.append(s)\n",
    "                elif s['titles'] is not None:\n",
    "                    for t in s['titles']:\n",
    "                        if name in t + ' ' + s[key]:\n",
    "                            candidates.append(s)\n",
    "        else:\n",
    "            if equal and s['aliases'] is not None:\n",
    "                for a in s['aliases']:\n",
    "                    if name == a:\n",
    "                        candidates.append(s)\n",
    "            elif not equal and s['aliases'] is not None:\n",
    "                for a in s['aliases']:\n",
    "                    if name in a:\n",
    "                        candidates.append(s)\n",
    "    # at equal level, we give priority to lenghty wikipedia (normalize) * links (normalize)\n",
    "    if len(candidates)>0:\n",
    "        mvp = sorted(candidates, key=lambda c: c['score'], reverse=True)[0]\n",
    "        return mvp['common_name']\n",
    "    else:\n",
    "        None\n",
    "\n",
    "def lemmatize(name, book):\n",
    "    shortened = []\n",
    "    for c in characters:\n",
    "        if c['books'] is not None:\n",
    "            if book in c['books']:\n",
    "                shortened.append(c)\n",
    "    \n",
    "    priority_fields = ['common_name', 'short_name', 'fullname', 'aliases']\n",
    "    priority_equality = [True, False]\n",
    "    priority_title = [False, True]\n",
    "    \n",
    "    for pf in priority_fields:\n",
    "        for pe in priority_equality:\n",
    "            for pt in priority_title:\n",
    "                candidate = getMVP(name, shortened, pf, pe, pt)\n",
    "                if candidate is not None:\n",
    "                    return candidate\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jorah Mormont\n",
      "Eddard Stark\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize('Ser Jorah',1))\n",
    "print(lemmatize('Ned',1))\n",
    "# Edric Dayne\n",
    "# Eddard Stark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the graph using lemmas mentionned in the same chapter\n",
    "The threshold is the max allowed distance between two lemmas in the text. Typically 500 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def addCharactersToGraph(characters, graph, threshold):\n",
    "    graph.add_nodes_from(characters.keys())\n",
    "    pairs = itertools.combinations(characters.keys(),2) \n",
    "    for u,v in pairs:\n",
    "        weight = 0\n",
    "        for ui in characters[u]['positions']:\n",
    "            for vi in characters[v]['positions']:\n",
    "                if abs(vi-ui) < threshold:\n",
    "                    weight+=1\n",
    "        if weight > 0:\n",
    "            if (u,v) not in graph.edges():\n",
    "                graph.add_edge(u,v,weight=weight)\n",
    "            else:\n",
    "                graph[u][v]['weight'] = graph[u][v]['weight'] + weight\n",
    "            \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each Chapter, we analyze the entities and prepare the sentences\n",
    "Each chapter contains some Persons, that might be characters. We save them and their position together with the book they belong to. \n",
    "This will allow us to \n",
    "* get a distance between persons\n",
    "* disambiguate the characters using the book they appear in to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 1211\n",
      "Number of edges: 27720\n",
      "Average degree:  45.7803\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import networkx as nx\n",
    "\n",
    "STOPLIST = set([\"n't\", \"'s\", \"'m\", \"ca\"] + list(STOP_WORDS))\n",
    "#SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", '\\t','\\n']\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentences = []\n",
    "G = nx.Graph()\n",
    "\n",
    "for i in range(len(chapters)):\n",
    "    #if i > 2:\n",
    "        #break\n",
    "        \n",
    "    chapterCharacters = {}\n",
    "    book, chapter = chapters[i]\n",
    "    chapter = chapter.strip().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    doc = nlp(chapter)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            entity = ent.text.replace(\"'s\",'')\n",
    "            if entity.startswith('Maester'):\n",
    "                entity = entity.replace('Maester ','')\n",
    "            lemma = lemmatize(entity,book)\n",
    "            #print(entity,'->',lemma)\n",
    "            \n",
    "            if lemma is not None:\n",
    "                if lemma not in chapterCharacters.keys():\n",
    "                    chapterCharacters[lemma] = {'books':set(),'positions':[]}\n",
    "                chapterCharacters[lemma]['books'].add(book)\n",
    "                chapterCharacters[lemma]['positions'].append(ent.start)\n",
    "            ent.merge(ent.root.tag_,lemma if lemma is not None else ent.text, ent.label_)\n",
    "    \n",
    "    sentence = []\n",
    "    for token in doc:\n",
    "       #print(token.ent_type_, token.text, token.pos_)\n",
    "        if token.ent_type_ == 'PERSON':\n",
    "            text = token.text.replace(' ', '_').replace(\"'s\",'')\n",
    "            tag = token.ent_type_\n",
    "            sentence.append('%s|%s' % (text, tag))\n",
    "        elif token.pos_ not in ['PUNCT','SPACE']:\n",
    "            sentence.append(token.lemma_.strip() if token.lemma_ != \"-PRON-\" else token.lower_)\n",
    "        elif token.pos_ == 'PUNCT' and token.text == '.':\n",
    "            #sentences.append(sentence)\n",
    "            sentences.append([word for word in sentence if word not in STOPLIST])\n",
    "            sentence = []\n",
    "    \n",
    "    #print(chapterCharacters)\n",
    "    G = addCharactersToGraph(chapterCharacters,G,500)\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G,'GOT-characters-NLP.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a gensim word2vec model from GOT books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Tywin_Lannister|PERSON', 0.9669781923294067),\n",
       " ('Arnolf|PERSON', 0.9654096364974976),\n",
       " ('Hoster_Tully|PERSON', 0.9652721285820007),\n",
       " ('Brandon|PERSON', 0.9609426259994507),\n",
       " ('protector', 0.9608691334724426),\n",
       " ('brightwater', 0.9599543809890747),\n",
       " ('Vale|PERSON', 0.9597084522247314),\n",
       " ('grandfather', 0.9590954184532166),\n",
       " ('foster', 0.9562659859657288),\n",
       " ('liege', 0.9560325145721436)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences, size=50, window=5, min_count=1, workers=4)\n",
    "model.wv.most_similar('Eddard_Stark|PERSON')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
