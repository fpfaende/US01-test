{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets for Characters lemmatization\n",
    "There are numerous Characters in Game of Thrones. The text often refers to them using only their firstname. We want to get a lemma of each name so this can be used in the graph later. The goal is to find the most probable lemma according to the mention in the text. \n",
    "\n",
    "To get the lemma, we need a list of characters. To date the best list is in [the wiki of ice and fire](https://awoiaf.westeros.org/index.php/List_of_characters)\n",
    "\n",
    "### Scrape the characters list page\n",
    "the scraping returns the text of the page and we load it into beautiful soup to have a DOM mount of it. \n",
    "We extract all links to character's page from the wiki and get all pages.\n",
    "Save all HTML for further analysis\n",
    "\n",
    "### get all the link that point to a character in the wiki\n",
    "the get the links we browse the HTML looking for \n",
    "\n",
    "```html\n",
    "    <a href=\"\">name</a>\n",
    "```\n",
    "tag and get only those who follow a certain template. \n",
    "Then we have a list of pages to get as tuples (href, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time \n",
    "\n",
    "# requests to the awoif wiki list of characters\n",
    "r = requests.get('https://awoiaf.westeros.org/index.php/List_of_characters')\n",
    "soup = BeautifulSoup(r.text)\n",
    "\n",
    "# extract all anchors (links). Take only the first anchor after an li\n",
    "links = soup.select('div#mw-content-text ul > li > a:nth-of-type(1)')\n",
    "pages = []\n",
    "for link in links:\n",
    "    # create a tuple with the link and the character name\n",
    "    pages.append((link['href'],link['href'].split('/')[-1]))\n",
    "\n",
    "# create a set to remove duplicates\n",
    "pages = set(pages)\n",
    "\n",
    "# add missing pages the wiki did not properly linked\n",
    "pages.add(('/index.php/Rhaenyra_Targaryen','Rhaenyra_Targaryen'))\n",
    "pages.add(('/index.php/Jason_Lannister_(son_of_Gerold)','Jason_Lannister_(son_of_Gerold)'))\n",
    "pages.add(('/index.php/Maekar_I_Targaryen','Maekar_I_Targaryen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start downloading all html files\n",
    "To handle errors and connection timeout, we download only those who hase not been downloaded yet. \n",
    "\n",
    "The faster way to handle this is to create use the difference between two sets\n",
    "* __downloaded__ set contains the files already in the directory data/html/\n",
    "* __pages__ set contains the whole list \n",
    "* __pending__ set contains __pages__-__downloaded__\n",
    "Pending are then downloaded one by one with a security pause of 1 sec between each request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "still 1 pages to download. Estimated: 0.016666666666666666 min\n"
     ]
    }
   ],
   "source": [
    "downloaded = set()\n",
    "for root, dirs, files in os.walk('data/html'):\n",
    "    for name in files:\n",
    "        if name.endswith('.html'):\n",
    "            url = '/index.php/'+name[:-5]\n",
    "            downloaded.add((url,name[:-5]))\n",
    "pending = pages - downloaded\n",
    "print('still',len(pending),'pages to download. Estimated:',len(pending)*2/60,'min' )\n",
    "\n",
    "# for each link, download the page and store on disk\n",
    "for href,name in pending:\n",
    "    r = requests.get('https://awoiaf.westeros.org' + href)\n",
    "    with open('data/html/'+ name+'.html','w+', encoding='utf-8') as fp:\n",
    "        fp.write(r.text)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analysis of html page: getCharacterDict\n",
    "We are after several informations:\n",
    "* Short name (title of the infobox)\n",
    "* common name (title of the __page__)\n",
    "* full name (field of the infobox)\n",
    "* title (field of the infobox)\n",
    "* aliases (field of the infobox) \n",
    "* book list (field of the infobox)\n",
    "* infobox items count (field of the infobox)\n",
    "* category (how character appears in the book, books field) \n",
    "\n",
    "if no infobox is found, the character is discarded\n",
    "\n",
    "The code use a variety of technics to extract, clean and array each field\n",
    "Biggest technical difficulty is the cleaning code:\n",
    "* aliases, title and books in the format `'BranStark (appears)[N 1]'` that needs to be transformed into `['Bran', 'Sark']`. This is achieved with regular expressions matching certain patterns in the text. An example of the regex used in this example is available on [regex101.com](https://regex101.com/r/5gYvlQ/1).\n",
    "* books volume title is converted into a number with the use of a dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanedAndArrayed(rawText):\n",
    "    \"\"\"\n",
    "    clean and array the scrapped raw text\n",
    "\n",
    "    First cleaning: use a pattern regular expression to suppress parenthesis and references field. \n",
    "    Second Splitting: split cleaned text into items when the patterns lowercaseUPPERCASE is found. \n",
    "\n",
    "    Parameters\n",
    "    —————\n",
    "    rawText : the raw text from infobox  \n",
    "\n",
    "    Returns\n",
    "    ———\n",
    "    array of string items\n",
    "        the cleaned and arrayed rawText\n",
    "    \"\"\"\n",
    "    \n",
    "    # transform: Ser (a title from ancient kingdoms)[N 1]Prince -> SerPrince\n",
    "    regex = r\"((\\[([a-zA-Z0-9]\\s?)+\\])|(\\s?\\(([a-zA-Z0-9]\\s?)+\\)))\"\n",
    "    subst = ''\n",
    "    cleaned = re.sub(regex, subst, rawText)\n",
    "    regex = r\"([a-z])([A-Z])\"\n",
    "    subst = \"\\\\1, \\\\2\"\n",
    "    cleaned = re.sub(regex, subst, cleaned)\n",
    "    # transform: SerPrince -> ['Ser', 'Prince']\n",
    "    arrayed = [i.strip() for i in cleaned.split(',')]\n",
    "    return arrayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_vol = {\n",
    "    'A Game of Thrones':1,\n",
    "    'A Clash of Kings':2,\n",
    "    'A Storm of Swords':3,\n",
    "    'A Feast for Crows':4,\n",
    "    'A Dance with Dragons':5\n",
    "}\n",
    "\n",
    "def getCharacterDict(soup):\n",
    "    \"\"\"\n",
    "    extract informations from page html extract\n",
    "\n",
    "    Check for every field required for testing names in the nlp analysis\n",
    "    Field is identified in the infobox or entire page, cleaned and arrayed\n",
    "    The character is discarded if not mentioned in the books we have\n",
    "\n",
    "    Parameters\n",
    "    —————\n",
    "    soup : BeautifulSoup html soup containng the whole awoif page \n",
    "\n",
    "    Returns\n",
    "    ———\n",
    "    dict of characters properties\n",
    "    \"\"\"\n",
    "    \n",
    "    fullname = None\n",
    "    aliases = None\n",
    "    titles = None\n",
    "    category=None\n",
    "    books = []\n",
    "    \n",
    "    # the COMMON NAME – PAGE\n",
    "    try:\n",
    "        common_name = soup.find('h1', class_='firstHeading').text.strip()\n",
    "    except:\n",
    "        common_name = None        \n",
    "\n",
    "    infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "    if infobox is None:\n",
    "        return None\n",
    "    \n",
    "    # the SHORT NAME – INFOBOX\n",
    "    if infobox.caption is not None:\n",
    "        short_name = infobox.caption.text.strip()\n",
    "    else:\n",
    "        short_name = infobox.tbody.tr.th.text.strip()\n",
    "        \n",
    "    rows = infobox.tbody.find_all(\"tr\")\n",
    "\n",
    "    # infobox fields for as a ranking function\n",
    "    td_count = 0\n",
    "    for row in rows:\n",
    "        for child in row.children:\n",
    "            if child.name == 'th':\n",
    "                tds = child.find_all('td')\n",
    "                td_count += len(tds)\n",
    "                # the fullname is a longer version of the common name (title of the page)\n",
    "                if row.th.text.lower().strip() =='full name':\n",
    "                    fullname = row.td.text.strip()\n",
    "\n",
    "                # title is sometimes used with the firstname. We keep it to try for title + common_name \n",
    "                if row.th.text.lower().strip() == 'title':\n",
    "                    raw = row.td.text.strip()\n",
    "                    titles = cleanedAndArrayed(raw)\n",
    "                \n",
    "                # aliases are other references to a common name\n",
    "                if row.th.text.lower().strip() =='alias':\n",
    "                    raw = row.td.text.strip()\n",
    "                    aliases = cleanedAndArrayed(raw)\n",
    "                \n",
    "                # we transform books titles into indexes corresponding to our chapter indexes\n",
    "                if row.th.text.lower().strip() == 'book(s)' or row.th.text.lower().strip() == 'books':\n",
    "                    raw = row.td.text.strip()\n",
    "                    \n",
    "                    categories={}\n",
    "                    if '(POV)' in raw:\n",
    "                        category = 'POV'\n",
    "                    else:\n",
    "                        categories['appears'] = raw.count('(appears)')\n",
    "                        categories['mentioned'] = raw.count('(mentioned)')\n",
    "                        categories['appendix'] = raw.count('(appendix)')\n",
    "                        category = sorted(categories.items(), key=lambda kv:kv[1], reverse=True)[0][0]\n",
    "                    # if more appendix -> appendix\n",
    "                    # if more mentioned -> mention\n",
    "                    \n",
    "                    books = cleanedAndArrayed(raw)\n",
    "                    books = [books_vol[b] for b in books if b in books_vol.keys()]\n",
    "\n",
    "    # if no books were found, the character is discarded           \n",
    "    if len(books) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return {'short_name':short_name, 'common_name':common_name, 'fullname':fullname, 'titles':titles,'aliases':aliases, 'books':books, 'awoif_infobox_length':td_count, 'category':category}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function definition : get characters internal links\n",
    "for graph creation purposes we save all links from one character into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCharacterLinks(soup,names):\n",
    "    cLinks = []\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        if 'href' in link.attrs.keys():\n",
    "            name = link['href'].split('/')[-1]\n",
    "            if name in names:\n",
    "                cLinks.append(name)\n",
    "    return cLinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper function display: display formatetd informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(character):\n",
    "    print(character['common_name'])\n",
    "    print('\\t',character['titles'])\n",
    "    print('\\t',character['aliases'])\n",
    "    print('\\t',character['books'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop\n",
    "\n",
    "We read the whole directory data/html containing the files. \n",
    "For each html file we mount the DOM and pass it to the two functions to get characters informations as well as links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "characters = []\n",
    "names = []\n",
    "\n",
    "for root, dirs, files in os.walk('data/html'):\n",
    "    for name in files:\n",
    "        if name.endswith('.html'):\n",
    "            names.append(name[:-5])\n",
    "\n",
    "    for name in files:\n",
    "        if name.endswith('.html'):\n",
    "            with open(os.path.join(root, name),'r') as fp:\n",
    "                html = fp.read()\n",
    "                soup = BeautifulSoup(html,'lxml')\n",
    "                character = getCharacterDict(soup)\n",
    "                if character is not None:\n",
    "                    #display(character)\n",
    "                    character['url'] = name[:-5]\n",
    "                    character['awoif_links'] = getCharacterLinks(soup,names)\n",
    "                    character['awoif_page_size'] = os.path.getsize(os.path.join(root, name))\n",
    "                    characters.append(character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some fields are created to allow future ranking function\n",
    "* **awoif_links**: internal links with other characters\n",
    "* **awoif_infobox_length**: count the field of infobox\n",
    "* **awoif_page_size**: size of the page in bytes \n",
    "* **awoif_infobox_length_norm**: count the field of infobox\n",
    "* **awoif_page_size_norm**: size of the page in bytes \n",
    "\n",
    "the normalization is created in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a graph to get in and out degree of every character\n",
    "import networkx as nx\n",
    "\n",
    "awoif_graph = nx.DiGraph()\n",
    "awoif_graph.add_nodes_from([c['url'] for c in characters])\n",
    "\n",
    "for character in characters:\n",
    "    for link in character['awoif_links']:\n",
    "        awoif_graph.add_edge(character['url'],link)\n",
    "\n",
    "for i in range(len(characters)):\n",
    "    characters[i]['awoif_in_degree'] = awoif_graph.in_degree(characters[i]['url'])\n",
    "    characters[i]['awoif_out_degree'] = awoif_graph.out_degree(characters[i]['url'])\n",
    "nx.write_gexf(G=awoif_graph,path='awoif-graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial import distance\n",
    "\n",
    "df  = pandas.DataFrame.from_dict(characters)\n",
    "scaler = MinMaxScaler()\n",
    "score = scaler.fit_transform(df[['awoif_infobox_length','awoif_page_size','awoif_in_degree','awoif_out_degree']])\n",
    "df['score'] = [distance.euclidean(s,[0,0,0,0]) for s in score]\n",
    "characters = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dictionaries as a pickle for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('characters.pickle','wb+') as fp:\n",
    "    pickle.dump(characters, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aliases': ['Lamprey'], 'awoif_in_degree': 0, 'awoif_infobox_length': 0, 'awoif_links': ['Davos_Seaworth'], 'awoif_out_degree': 1, 'awoif_page_size': 21753, 'books': [3], 'category': 'appears', 'common_name': 'Lamprey', 'fullname': None, 'short_name': 'Lamprey', 'titles': None, 'url': 'Lamprey', 'score': 0.009683657050394371}\n",
      "{'aliases': None, 'awoif_in_degree': 3, 'awoif_infobox_length': 0, 'awoif_links': ['Esgred', 'Sigrin', 'Theon_Greyjoy', 'Asha_Greyjoy'], 'awoif_out_degree': 4, 'awoif_page_size': 20590, 'books': [2], 'category': 'mentioned', 'common_name': 'Esgred', 'fullname': None, 'short_name': 'Esgred', 'titles': None, 'url': 'Esgred', 'score': 0.023760379484546516}\n",
      "{'aliases': None, 'awoif_in_degree': 0, 'awoif_infobox_length': 0, 'awoif_links': ['Aeron_Greyjoy', 'Euron_Greyjoy'], 'awoif_out_degree': 2, 'awoif_page_size': 22303, 'books': [4, 5], 'category': 'mentioned', 'common_name': 'Old Grey Gull', 'fullname': None, 'short_name': 'Old Grey Gull', 'titles': None, 'url': 'Old_Grey_Gull', 'score': 0.01440325526544506}\n"
     ]
    }
   ],
   "source": [
    "print(characters[1])\n",
    "print(characters[10])\n",
    "print(characters[200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
