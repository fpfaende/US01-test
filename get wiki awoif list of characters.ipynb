{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets for Characters lemmatization\n",
    "There are numerous Characters in Game of Thrones. The text often refers to them using only their firstname. We want to get a lemma of each name so this can be used in the graph later. The goal is to find the most probable lemma according to the mention in the text. \n",
    "\n",
    "To get the lemma, we need a list of characters. To date the best list is in [the wiki of ice and fire](https://awoiaf.westeros.org/index.php/List_of_characters)\n",
    "\n",
    "### Scrape the characters list page\n",
    "the scraping returns the text of the page and we load it into beautiful soup to have a DOM mount of it. \n",
    "We extract all links to character's page from the wiki and get all pages.\n",
    "Save all HTML for further analysis\n",
    "\n",
    "### get all the link that point to a character in the wiki\n",
    "the get the links we browse the HTML looking for \n",
    "\n",
    "```html\n",
    "    <a href=\"\">name</a>\n",
    "```\n",
    "tag and get only those who follow a certain template. \n",
    "Then we have a list of pages to get as tuples (href, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/index.php/Aegon_V_Targaryen\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# requests to the \n",
    "r = requests.get('https://awoiaf.westeros.org/index.php/List_of_characters')\n",
    "soup = BeautifulSoup(r.text)\n",
    "\n",
    "# extract all anchors (links) tags: 'a'\n",
    "links = soup.find_all('a')\n",
    "pages = []\n",
    "for link in links:\n",
    "    S1 = set(['title','href'])\n",
    "    S2 = set(link.attrs.keys())\n",
    "    if S1 == S2:\n",
    "        pages.append((link['href'],link['href'].split('/')[-1]))\n",
    "\n",
    "pages = pages[:-5] # remove last 5 as they are irrelevant\n",
    "pages = set(pages) # create a set to remove duplicates\n",
    "\n",
    "# for each link, download the page and store on disk\n",
    "for href,name in pages:\n",
    "    r = requests.get('https://awoiaf.westeros.org' + href)\n",
    "    with open(name+'.html','w+', encoding='utf-8') as fp:\n",
    "        fp.write(r.text)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analysis of html page: getCharacterDict\n",
    "We are after several informations:\n",
    "* Short name (title of the infobox)\n",
    "* common name (title of the page)\n",
    "* full name (field fo the infobox)\n",
    "* aliases (field of the infobox) \n",
    "* book list (field of the infobox)\n",
    "\n",
    "if no infobox was found, discard the character\n",
    "\n",
    "The code use a variety of technics. Most difficult code is for:\n",
    "* aliases in the format `'BranStark[1]'` that needs to be transformed into `['Bran', 'Sark']`\n",
    "* books in the same format but with parenthesis and volume title is converted into a number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_vol = {\n",
    "    'A Game of Thrones':1,\n",
    "    'A Clash of Kings':2,\n",
    "    'A Storm of Swords':3,\n",
    "    'A Feast for Crows':4,\n",
    "    'A Dance with Dragons':5\n",
    "}\n",
    "\n",
    "\n",
    "def getCharacterDict(soup):\n",
    "    fullname = None\n",
    "    aliases = None\n",
    "    books = None\n",
    "    \n",
    "    try:\n",
    "        short_name = soup.find(\"table\", class_=\"infobox\").caption.text.strip()\n",
    "    except:\n",
    "        short_name = None\n",
    "    \n",
    "    try:\n",
    "        common_name = soup.find('h1', class_='firstHeading').text.strip()\n",
    "    except:\n",
    "        common_name = None        \n",
    "\n",
    "    try:\n",
    "        tbody = soup.find(\"table\", class_=\"infobox\").tbody\n",
    "        rows = tbody.find_all(\"tr\")\n",
    "\n",
    "        for row in rows:\n",
    "            for child in row.children:\n",
    "                if child.name == 'th':\n",
    "                    if row.th.text.lower()=='full name':\n",
    "                        fullname = row.td.text.strip()\n",
    "\n",
    "                    if row.th.text.lower()=='alias':\n",
    "                        raw_alias = row.td.text\n",
    "                        regex = r\"(\\s?\\[[0-9]+\\])\"\n",
    "                        subst = ''\n",
    "                        remove_ref_aliases = re.sub(regex, subst, raw_alias)\n",
    "                        # detect aliases separation\n",
    "                        regex = r\"([a-z])([A-Z])\"\n",
    "                        subst = \"\\\\1, \\\\2\"\n",
    "                        normalized_alias = re.sub(regex, subst, remove_ref_aliases)\n",
    "                        # recreate an array with the aliases\n",
    "                        aliases = [t.strip() for t in normalized_alias.split(',')]\n",
    "\n",
    "                    if row.th.text.lower() == 'book(s)':\n",
    "                        raw_books = row.td.text\n",
    "                        regex = r\"(\\s\\([a-zA-Z]+\\))\"\n",
    "                        subst = ''\n",
    "                        remove_parenthesis_books = re.sub(regex, subst, raw_books)\n",
    "                        regex = r\"([a-z])([A-Z])\"\n",
    "                        subst = \"\\\\1, \\\\2\"\n",
    "                        normalized_books = re.sub(regex, subst, remove_parenthesis_books)\n",
    "                        books = [books_vol[b.strip()] for b in normalized_books.split(',') if b.strip() in books_vol.keys()]\n",
    "    except:\n",
    "        return None\n",
    "    return {'short_name':short_name, 'common_name':common_name, 'fullname':fullname, 'aliases':aliases, 'books':books}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function definition : get characters internal links\n",
    "for graph creation purposes we save all links from one character into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCharacterLinks(soup,names):\n",
    "    cLinks = []\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        if 'href' in link.attrs.keys():\n",
    "            name = link['href'].split('/')[-1]\n",
    "            if name in names:\n",
    "                cLinks.append(name)\n",
    "    return cLinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop\n",
    "\n",
    "We read the whole directory data/html containing the files. \n",
    "For each html file we mount the DOM and pass it to the two functions to get characters informations as well as links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "characters = []\n",
    "characters_links = []\n",
    "names = []\n",
    "for root, dirs, files in os.walk('data/html'):\n",
    "    for name in files:\n",
    "        if name.endswith('.html'):\n",
    "            names.append(name[:-5])\n",
    "\n",
    "    for name in files:\n",
    "        if name != 'Daenerys_Targaryen.html':\n",
    "            continue\n",
    "        if name.endswith('.html'):\n",
    "            with open(os.path.join(root, name),'r') as fp:\n",
    "                html = fp.read()\n",
    "                soup = BeautifulSoup(html,'lxml')\n",
    "                character = getCharacterDict(soup)\n",
    "                if character is not None:\n",
    "                    character['url'] = name[:-5]\n",
    "                    characters.append(character)\n",
    "                    character_links = {name[:-5]:getCharacterLinks(soup,names)}\n",
    "                    characters_links.append(character_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dictionaries as a pickle for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('characters.pickle','wb+') as fp:\n",
    "    pickle.dump(characters, fp)\n",
    "with open('characters_links.pickle','wb+') as fp:\n",
    "    pickle.dump(characters_links, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
