{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets for Characters lemmatization\n",
    "There are numerous Characters in Game of Thrones. The text often refers to them using only their firstname. We want to get a lemma of each name so this can be used in the graph later. The goal is to find the most probable lemma according to the mention in the text. \n",
    "\n",
    "To get the lemma, we need a list of characters. To date the best list is in [the wiki of ice and fire](https://awoiaf.westeros.org/index.php/List_of_characters)\n",
    "\n",
    "### Scrape the characters list page\n",
    "the scraping returns the text of the page and we load it into beautiful soup to have a DOM mount of it. \n",
    "We extract all links to character's page from the wiki and get all pages.\n",
    "Save all HTML for further analysis\n",
    "\n",
    "### get all the link that point to a character in the wiki\n",
    "the get the links we browse the HTML looking for \n",
    "\n",
    "```html\n",
    "    <a href=\"\">name</a>\n",
    "```\n",
    "tag and get only those who follow a certain template. \n",
    "Then we have a list of pages to get as tuples (href, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "still 1 pages to download. Estimated: 0.016666666666666666 min\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time \n",
    "\n",
    "# requests to the \n",
    "r = requests.get('https://awoiaf.westeros.org/index.php/List_of_characters')\n",
    "soup = BeautifulSoup(r.text)\n",
    "\n",
    "# extract all anchors (links) tags: 'a'\n",
    "links = soup.select('div#mw-content-text ul > li > a:nth-of-type(1)')\n",
    "pages = []\n",
    "for link in links:\n",
    "    pages.append((link['href'],link['href'].split('/')[-1]))\n",
    "\n",
    "pages = set(pages) # create a set to remove duplicates\n",
    "pages.add(('/index.php/Rhaenyra_Targaryen','Rhaenyra_Targaryen'))\n",
    "pages.add(('/index.php/Jason_Lannister_(son_of_Gerold)','Jason_Lannister_(son_of_Gerold)'))\n",
    "pages.add(('/index.php/Maekar_I_Targaryen','Maekar_I_Targaryen'))\n",
    "\n",
    "downloaded = set()\n",
    "for root, dirs, files in os.walk('data/html'):\n",
    "    for name in files:\n",
    "        if name.endswith('.html'):\n",
    "            url = '/index.php/'+name[:-5]\n",
    "            downloaded.add((url,name[:-5]))\n",
    "pending = pages - downloaded\n",
    "print('still',len(pending),'pages to download. Estimated:',len(pending)/60,'min' )\n",
    "\n",
    "# for each link, download the page and store on disk\n",
    "for href,name in pending:\n",
    "    r = requests.get('https://awoiaf.westeros.org' + href)\n",
    "    with open('data/html/'+ name+'.html','w+', encoding='utf-8') as fp:\n",
    "        fp.write(r.text)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analysis of html page: getCharacterDict\n",
    "We are after several informations:\n",
    "* Short name (title of the infobox)\n",
    "* common name (title of the page)\n",
    "* full name (field fo the infobox)\n",
    "* aliases (field of the infobox) \n",
    "* book list (field of the infobox)\n",
    "\n",
    "if no infobox was found, discard the character\n",
    "\n",
    "The code use a variety of technics. Most difficult code is for:\n",
    "* aliases, title and books in the format `'BranStark (appears)[N 1]'` that needs to be transformed into `['Bran', 'Sark']`\n",
    "* books volume title is converted into a number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_vol = {\n",
    "    'A Game of Thrones':1,\n",
    "    'A Clash of Kings':2,\n",
    "    'A Storm of Swords':3,\n",
    "    'A Feast for Crows':4,\n",
    "    'A Dance with Dragons':5\n",
    "}\n",
    "\n",
    "# helper function to help clean scrapped text with regular expressions\n",
    "# you can try the regex here: https://regex101.com/r/5gYvlQ/1\n",
    "def cleanedAndArrayed(rawText):\n",
    "    # transform: Ser (a title from ancient kingdoms)[N 1]Prince -> SerPrince\n",
    "    regex = r\"((\\[([a-zA-Z0-9]\\s?)+\\])|(\\s?\\(([a-zA-Z0-9]\\s?)+\\)))\"\n",
    "    subst = ''\n",
    "    cleaned = re.sub(regex, subst, rawText)\n",
    "    regex = r\"([a-z])([A-Z])\"\n",
    "    subst = \"\\\\1, \\\\2\"\n",
    "    cleaned = re.sub(regex, subst, cleaned)\n",
    "    # transform: SerPrince -> ['Ser', 'Prince']\n",
    "    arrayed = [i.strip() for i in cleaned.split(',')]\n",
    "    return arrayed\n",
    "\n",
    "\n",
    "def getCharacterDict(soup):\n",
    "    fullname = None\n",
    "    aliases = None\n",
    "    books = []\n",
    "    titles = None\n",
    "    \n",
    "    try:\n",
    "        common_name = soup.find('h1', class_='firstHeading').text.strip()\n",
    "    except:\n",
    "        common_name = None        \n",
    "\n",
    "    infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "    if infobox is None:\n",
    "        return None\n",
    "    \n",
    "    if infobox.caption is not None:\n",
    "        short_name = infobox.caption.text.strip()\n",
    "    else:\n",
    "        short_name = infobox.tbody.tr.th.text.strip()\n",
    "        \n",
    "    rows = infobox.tbody.find_all(\"tr\")\n",
    "\n",
    "    # we keep a count of the infobox fields for as a ranking function\n",
    "    th_count = 0\n",
    "    for row in rows:\n",
    "        for child in row.children:\n",
    "            if child.name == 'th':\n",
    "                th_count +=1\n",
    "                # the fullname is a longer version of the common name (title of the page)\n",
    "                if row.th.text.lower().strip() =='full name':\n",
    "                    fullname = row.td.text.strip()\n",
    "\n",
    "                # title is sometimes used with the firstname. We keep it to try for title + common_name \n",
    "                if row.th.text.lower().strip() == 'title':\n",
    "                    raw = row.td.text.strip()\n",
    "                    titles = cleanedAndArrayed(raw)\n",
    "                \n",
    "                # aliases are other references to a common name\n",
    "                if row.th.text.lower().strip() =='alias':\n",
    "                    raw = row.td.text.strip()\n",
    "                    aliases = cleanedAndArrayed(raw)\n",
    "                \n",
    "                # we transform books titles into indexes corresponding to our chapter indexes\n",
    "                if row.th.text.lower().strip() == 'book(s)' or row.th.text.lower().strip() == 'books':\n",
    "                    raw = row.td.text.strip()\n",
    "                    books = cleanedAndArrayed(raw)\n",
    "                    books = [books_vol[b] for b in books if b in books_vol.keys()]\n",
    "\n",
    "    # if no books were found, the character is discarded           \n",
    "    if len(books) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return {'short_name':short_name, 'common_name':common_name, 'fullname':fullname, 'titles':titles,'aliases':aliases, 'books':books, 'awoif_infobox_length':th_count}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function definition : get characters internal links\n",
    "for graph creation purposes we save all links from one character into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCharacterLinks(soup,names):\n",
    "    cLinks = []\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        if 'href' in link.attrs.keys():\n",
    "            name = link['href'].split('/')[-1]\n",
    "            if name in names:\n",
    "                cLinks.append(name)\n",
    "    return cLinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper function display: display formatetd informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(character):\n",
    "    print(character['common_name'])\n",
    "    print('\\t',character['titles'])\n",
    "    print('\\t',character['aliases'])\n",
    "    print('\\t',character['books'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop\n",
    "\n",
    "We read the whole directory data/html containing the files. \n",
    "For each html file we mount the DOM and pass it to the two functions to get characters informations as well as links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "characters = []\n",
    "names = []\n",
    "\n",
    "for root, dirs, files in os.walk('data/html'):\n",
    "    for name in files:\n",
    "        if name.endswith('.html'):\n",
    "            names.append(name[:-5])\n",
    "\n",
    "    for name in files:\n",
    "        if name.endswith('.html'):\n",
    "            with open(os.path.join(root, name),'r') as fp:\n",
    "                html = fp.read()\n",
    "                soup = BeautifulSoup(html,'lxml')\n",
    "                character = getCharacterDict(soup)\n",
    "                if character is not None:\n",
    "                    #display(character)\n",
    "                    character['url'] = name[:-5]\n",
    "                    character['awoif_links'] = getCharacterLinks(soup,names)\n",
    "                    character['awoif_page_size'] = os.path.getsize(os.path.join(root, name))\n",
    "                    characters.append(character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some fields are created to allow future ranking function\n",
    "* **awoif_links**: internal links with other characters\n",
    "* **awoif_infobox_length**: count the field of infobox\n",
    "* **awoif_page_size**: size of the page in bytes \n",
    "* **awoif_infobox_length_norm**: count the field of infobox\n",
    "* **awoif_page_size_norm**: size of the page in bytes \n",
    "\n",
    "the normalization is created in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a graph to get in and out degree of every character\n",
    "import networkx as nx\n",
    "\n",
    "awoif_graph = nx.DiGraph()\n",
    "awoif_graph.add_nodes_from([c['url'] for c in characters])\n",
    "\n",
    "for character in characters:\n",
    "    for link in character['awoif_links']:\n",
    "        awoif_graph.add_edge(character['url'],link)\n",
    "\n",
    "for i in range(len(characters)):\n",
    "    characters[i]['awoif_in_degree'] = awoif_graph.in_degree(characters[i]['url'])\n",
    "    characters[i]['awoif_out_degree'] = awoif_graph.out_degree(characters[i]['url'])\n",
    "nx.write_gexf(G=awoif_graph,path='awoif-graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15789474 0.01606075 0.0228013  0.03174603]\n",
      " [0.15789474 0.00811039 0.         0.00529101]\n",
      " [0.21052632 0.14710127 0.04885993 0.08465608]\n",
      " ...\n",
      " [0.10526316 0.02994444 0.01302932 0.02645503]\n",
      " [0.05263158 0.01249774 0.00977199 0.        ]\n",
      " [0.15789474 0.01445136 0.00325733 0.01587302]]\n",
      "[0.79709898 0.13658178 0.04348276 0.02283648]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.spatial import distance\n",
    "\n",
    "df  = pandas.DataFrame.from_dict(characters)\n",
    "scaler = MinMaxScaler()\n",
    "score = scaler.fit_transform(df[['awoif_infobox_length','awoif_page_size','awoif_in_degree','awoif_out_degree']])\n",
    "print(score)\n",
    "\n",
    "\n",
    "score2 = StandardScaler().fit_transform(df[['awoif_infobox_length','awoif_page_size','awoif_in_degree','awoif_out_degree']])\n",
    "\n",
    "df['score'] = [distance.euclidean(s,[0,0,0,0]) for s in score]\n",
    "characters = df.to_dict(orient='records')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=4)\n",
    "principalComponents = pca.fit_transform(score2)\n",
    "principalDf = pandas.DataFrame(data = principalComponents, columns = ['pc1', 'pc2', 'pc3', 'pc4'])\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dictionaries as a pickle for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('characters.pickle','wb+') as fp:\n",
    "    pickle.dump(characters, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
