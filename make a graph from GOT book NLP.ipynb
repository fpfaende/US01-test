{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAME OF THRONES: A GRAPH EXPERIMENT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the extracted characters informations from pickles\n",
    "\n",
    "The dictionaries are loaded directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('characters.pickle','rb') as fp:\n",
    "    characters = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'short_name': 'Daenerys Targaryen', 'common_name': 'Daenerys Targaryen', 'fullname': 'Daenerys Targaryen the First of Her Name', 'aliases': ['Daenerys Stormborn', 'Dany', 'The Unburnt', 'Mhysa', 'The silver queen', 'Silver Lady', 'Dragonmother', 'The dragon queen', 'Breaker of Chains'], 'books': [1, 2, 3, 4, 5], 'url': 'Daenerys_Targaryen'}]\n"
     ]
    }
   ],
   "source": [
    "print(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('characters_links.pickle','rb') as fp:\n",
    "    character_links = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the chapters and the book they belong to\n",
    "The books have been cut into chapters themselves into a book directory named GOT{x} where x is the book in order of publication.\n",
    "We read the whole directory to retain only text files. we create a chapter list containing the text of the chapter and the book index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 345 chapters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "\n",
    "chapters = []\n",
    "\n",
    "for root, dirs, files in os.walk('data'):\n",
    "    for name in files:\n",
    "        if name.endswith('txt'):\n",
    "            book = int(root.split('/')[-1][-1:])\n",
    "            with open(join(root,name),'r') as fp:\n",
    "                chapters.append((book,fp.read()))\n",
    "                \n",
    "print('found',len(chapters),'chapters')\n",
    "#print(chapters[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each Chapter, we analyze the entities and prepare the sentences\n",
    "Each chapter contains some Persons, that might be characters. We save them and their position together with the book they belong to. \n",
    "This will allow us to \n",
    "* get a distance between persons\n",
    "* disambiguate the characters using the book they appear in to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCandidates(name, shortened, key, equal=True):\n",
    "    candidates = []\n",
    "    print(name, key, equal)\n",
    "    for s in shortened:\n",
    "        if key != 'aliases':\n",
    "            if equal and s[key] is not None:\n",
    "                if name == s[key]:\n",
    "                    candidates.append(s)\n",
    "            elif not equal and s[key] is not None:\n",
    "                if name in s[key]:\n",
    "                    candidates.append(s)\n",
    "        else:\n",
    "            if equal and s['aliases'] is not None:\n",
    "                for a in s['aliases']:\n",
    "                    if name == a:\n",
    "                        candidates.append(s)\n",
    "            elif not equal and s['aliases'] is not None:\n",
    "                for a in s['aliases']:\n",
    "                    if name in a:\n",
    "                        candidates.append(s)\n",
    "                \n",
    "    if len(candidates)>0:\n",
    "        #if len(candidates)>1:\n",
    "        #    mvp = sorted(candidates.items(), key=lambda kv: len(kv[1]['books']))[0]\n",
    "        #    return mvp['common_name']\n",
    "        #else:\n",
    "        return candidates[0]['common_name']\n",
    "    else:\n",
    "        None\n",
    "\n",
    "def lemmatize(name, book):\n",
    "    shortened = []\n",
    "    for c in characters:\n",
    "        if c['books'] is not None:\n",
    "            if book in c['books']:\n",
    "                shortened.append(c)\n",
    "    \n",
    "    priorities = ['common_name', 'short_name', 'fullname', 'aliases']\n",
    "    \n",
    "    for priority in priorities:\n",
    "        candidate = getCandidates(name, shortened, priority)\n",
    "        if candidate is not None:\n",
    "            return candidate\n",
    "    \n",
    "    for priority in priorities:\n",
    "        candidate = getCandidates(name, shortened, priority, equal=False)\n",
    "        if candidate is not None:\n",
    "            return candidate\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daenerys common_name True\n",
      "Daenerys short_name True\n",
      "Daenerys fullname True\n",
      "Daenerys aliases True\n",
      "Daenerys common_name False\n",
      "Daenerys short_name False\n",
      "Daenerys fullname False\n",
      "Daenerys aliases False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize('Daenerys',1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ned None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6231bc632dc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'PERSON'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'s\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcharacters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mcharacters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'books'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'positions'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c7de237c3a1e>\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(name, book)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mshortened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcharacters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mbook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'books'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mshortened\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import networkx as nx\n",
    "\n",
    "STOPLIST = set([\"n't\", \"'s\", \"'m\", \"ca\"] + list(STOP_WORDS))\n",
    "#SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", '\\t','\\n']\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "characters = {}\n",
    "sentences = [] \n",
    "for i in range(len(chapters)):\n",
    "    if i > 2:\n",
    "        break\n",
    "    book, chapter = chapters[i]\n",
    "    chapter = chapter.strip().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    doc = nlp(chapter)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            entity = ent.text.replace(\"'s\",'')\n",
    "            print(entity, lemmatize(entity,book))\n",
    "            if ent.text not in characters.keys():\n",
    "                characters[entity] = {'books':set(),'positions':[]}\n",
    "            \n",
    "            characters[entity]['books'].add(book)\n",
    "            characters[entity]['positions'].append((ent.start, ent.end))\n",
    "            ent.merge(ent.root.tag_,ent.text, ent.label_)\n",
    "    \n",
    "    sentence = []\n",
    "    for token in doc:\n",
    "       #print(token.ent_type_, token.text, token.pos_)\n",
    "        if token.ent_type_ == 'PERSON':\n",
    "            text = token.text.replace(' ', '_').replace(\"'s\",'')\n",
    "            tag = token.ent_type_\n",
    "            sentence.append('%s|%s' % (text, tag))\n",
    "        elif token.pos_ not in ['PUNCT','SPACE']:\n",
    "            sentence.append(token.lemma_.strip() if token.lemma_ != \"-PRON-\" else token.lower_)\n",
    "        elif token.pos_ == 'PUNCT' and token.text == '.':\n",
    "            sentences.append(sentence)\n",
    "            sentences.append([word for word in sentence if word not in STOPLIST])\n",
    "            sentence = []\n",
    "\n",
    "print(characters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('by', 0.9998288154602051),\n",
       " ('lannister', 0.9998278021812439),\n",
       " ('jon', 0.9998244643211365),\n",
       " ('Ned|PERSON', 0.9998223781585693),\n",
       " ('Robert|PERSON', 0.9998213052749634),\n",
       " ('brother', 0.9998184442520142),\n",
       " ('stone', 0.9998167157173157),\n",
       " ('old', 0.9998142719268799),\n",
       " ('great', 0.9998133778572083),\n",
       " ('child', 0.9998103380203247)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences, size=50, window=5, min_count=1, workers=4)\n",
    "model.wv.most_similar('Bran|PERSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "couples = list(itertools.combinations(characters, 2))\n",
    "G.add_edges_from(couples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 17\n",
      "Number of edges: 136\n",
      "Average degree:  16.0000\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(G))\n",
    "nx.write_gexf(G, 'one-chapter.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
